@techreport{Armbrust:EECS-2009-28,
    Author= {Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy H. and Konwinski, Andrew and Lee, Gunho and Patterson, David A. and Rabkin, Ariel and Stoica, Ion and Zaharia, Matei},
    Title= {Above the Clouds: A Berkeley View of Cloud Computing},
    Year= {2009},
    Month= {Feb},
    Url= {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-28.html},
    Number= {UCB/EECS-2009-28},
}

@InProceedings{Ustiugov2021,
  author     = {Ustiugov, Dmitrii and Petrov, Plamen and Kogias, Marios and Bugnion, Edouard and Grot, Boris},
  booktitle  = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  title      = {Benchmarking, analysis, and optimization of serverless function snapshots},
  year       = {2021},
  month      = apr,
  publisher  = {ACM},
  series     = {ASPLOS ’21},
  collection = {ASPLOS ’21},
  doi        = {10.1145/3445814.3446714},
}

@Misc{link-openfaas,
  title = {OpenFaaS makes it simple to deploy both functions and existing code to Kubernetes},
  url   = {https://www.openfaas.com},
}

@Misc{link-traefix,
  title = {Traefik: The Cloud NativeApplication Proxy},
  url   = {https://traefik.io/traefik/},
}

@InProceedings{Wu2024,
  author    = {Zhanghao Wu and Wei-Lin Chiang and Ziming Mao and Zongheng Yang and Eric Friedman and Scott Shenker and Ion Stoica},
  booktitle = {21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
  title     = {Can{\textquoteright}t Be Late: Optimizing Spot Instance Savings under Deadlines},
  year      = {2024},
  address   = {Santa Clara, CA},
  month     = apr,
  pages     = {185--203},
  publisher = {USENIX Association},
  isbn      = {978-1-939133-39-7},
  url       = {https://www.usenix.org/conference/nsdi24/presentation/wu-zhanghao},
}

@Misc{link-candy,
  title = {Caddy: The ultimate server},
  url   = {https://caddyserver.com},
}

@Misc{link-kind,
  title = {kind is a tool for running local Kubernetes clusters using Docker container “nodes”.},
  url   = {https://kind.sigs.k8s.io},
}

@InProceedings{Duplyakin2019,
  author    = {Dmitry Duplyakin and Robert Ricci and Aleksander Maricq and Gary Wong and Jonathon Duerig and Eric Eide and Leigh Stoller and Mike Hibler and David Johnson and Kirk Webb and Aditya Akella and Kuangching Wang and Glenn Ricart and Larry Landweber and Chip Elliott and Michael Zink and Emmanuel Cecchet and Snigdhaswin Kar and Prabodh Mishra},
  booktitle = {Proceedings of the {USENIX} Annual Technical Conference (ATC)},
  title     = {The Design and Operation of {CloudLab}},
  year      = {2019},
  month     = jul,
  pages     = {1--14},
  url       = {https://www.flux.utah.edu/paper/duplyakin-atc19},
}

@Misc{link-deathstarbench,
}

@InProceedings{Gan2019,
  author    = {Gan, Yu and Zhang, Yanqi and Cheng, Dailun and Shetty, Ankitha and Rathi, Priyal and Katarki, Nayan and Bruno, Ariana and Hu, Justin and Ritchken, Brian and Jackson, Brendon and Hu, Kelvin and Pancholi, Meghna and He, Yuan and Clancy, Brett and Colen, Chris and Wen, Fukang and Leung, Catherine and Wang, Siyuan and Zaruvinsky, Leon and Espinosa, Mateo and Lin, Rick and Liu, Zhongling and Padilla, Jake and Delimitrou, Christina},
  booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
  title     = {An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud \& Edge Systems},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {3–18},
  publisher = {Association for Computing Machinery},
  series    = {ASPLOS '19},
  abstract  = {Cloud services have recently started undergoing a major shift from monolithic applications, to graphs of hundreds or thousands of loosely-coupled microservices. Microservices fundamentally change a lot of assumptions current cloud systems are designed with, and present both opportunities and challenges when optimizing for quality of service (QoS) and cloud utilization.In this paper we explore the implications microservices have across the cloud system stack. We first present DeathStarBench, a novel, open-source benchmark suite built with microservices that is representative of large end-to-end services, modular and extensible. DeathStarBench includes a social network, a media service, an e-commerce site, a banking system, and IoT applications for coordination control of UAV swarms. We then use DeathStarBench to study the architectural characteristics of microservices, their implications in networking and operating systems, their challenges with respect to cluster management, and their trade-offs in terms of application design and programming frameworks. Finally, we explore the tail at scale effects of microservices in real deployments with hundreds of users, and highlight the increased pressure they put on performance predictability.},
  doi       = {10.1145/3297858.3304013},
  isbn      = {9781450362405},
  keywords  = {serverless, qos, microservices, fpga, datacenters, cluster management, cloud computing, acceleration},
  location  = {Providence, RI, USA},
  numpages  = {16},
  url       = {https://doi.org/10.1145/3297858.3304013},
}



@inproceedings{10.1145,
author = {He, Keqiang and Fisher, Alexis and Wang, Liang and Gember, Aaron and Akella, Aditya and Ristenpart, Thomas},
title = {Next stop, the cloud: understanding modern web service deployment in EC2 and azure},
year = {2013},
isbn = {9781450319539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2504730.2504740},
doi = {10.1145/2504730.2504740},
abstract = {An increasingly large fraction of Internet services are hosted on a cloud computing system such as Amazon EC2 or Windows Azure. But to date, no in-depth studies about cloud usage by Internet services has been performed. We provide a detailed measurement study to shed light on how modern web service deployments use the cloud and to identify ways in which cloud-using services might improve these deployments. Our results show that: 4\% of the Alexa top million use EC2/Azure; there exist several common deployment patterns for cloud-using web service front ends; and services can significantly improve their wide-area performance and failure tolerance by making better use of existing regional diversity in EC2. Driving these analyses are several new datasets, including one with over 34 million DNS records for Alexa websites and a packet capture from a large university network.},
booktitle = {Proceedings of the 2013 Conference on Internet Measurement Conference},
pages = {177–190},
numpages = {14},
keywords = {web service, trace analysis, cloud computing, azure, EC2, DNS},
location = {Barcelona, Spain},
series = {IMC '13}
}

@InProceedings{Li2019,
  author    = {Li, Junfeng and Kulkarni, Sameer G. and Ramakrishnan, K. K. and Li, Dan},
  booktitle = {Proceedings of the 5th International Workshop on Serverless Computing},
  title     = {Understanding Open Source Serverless Platforms: Design Considerations and Performance},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {37–42},
  publisher = {Association for Computing Machinery},
  series    = {WOSC '19},
  abstract  = {Serverless computing is increasingly popular because of the promise of lower cost and the convenience it provides to users who do not need to focus on server management. This has resulted in the availability of a number of proprietary and open-source serverless solutions. We seek to understand how the performance of serverless computing depends on a number of design issues using several popular open-source serverless platforms. We identify the idiosyncrasies affecting performance (throughput and latency) for different open-source serverless platforms. Further, we observe that just having either resource-based (CPU and memory) or workload-based (request per second (RPS) or concurrent requests) auto-scaling is inadequate to address the needs of the serverless platforms.},
  doi       = {10.1145/3366623.3368139},
  isbn      = {9781450370387},
  keywords  = {function-as-a-service, performance, serverless},
  location  = {Davis, CA, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3366623.3368139},
}

@InProceedings{Kaviani2019,
  author    = {Kaviani, Nima and Kalinin, Dmitriy and Maximilien, Michael},
  booktitle = {Proceedings of the 5th International Workshop on Serverless Computing},
  title     = {Towards Serverless as Commodity: a case of Knative},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {13–18},
  publisher = {Association for Computing Machinery},
  series    = {WOSC '19},
  abstract  = {Serverless computing promises to evolve cloud computing architecture from VMs and containers-as-a-service (CaaS) to function-as-a-service (FaaS). This takes away complexities of managing and scaling underlying infrastructure and can result in simpler code, cheaper realization of services, and higher availability. Nonetheless, one of the primary drawbacks customers face when making decision to move their software to a serverless platform is the potential for getting locked-in with a particular provider. This used to be a concern with Platform-as-a-Service (PaaS) offerings too. However with Kubernetes emerging as the industry standard PaaS layer, PaaS is closer to becoming commodity with the Kubernetes API as its common interface. The question is if a similar unification for the API interface layer and runtime contracts can be achieved for serverless. If achieved, this would free up serverless users from their fears of platform lock-in. Our goal in this paper is to extract a minimal common denominator model of execution that can move us closer to a unified serverless platform. As contributors to Knative [13] with in-depth understanding of its internal design, we use Knative as the baseline for this comparison and contrast its API interface and runtime contracts against other prominent serverless platforms to identify commonalities and differences. Influenced by the work in Knative, we also discuss challenges as well as the necessary evolution we expect to see as serverless platforms themselves reach commodity status.},
  doi       = {10.1145/3366623.3368135},
  isbn      = {9781450370387},
  keywords  = {cloud, performance, scalability, serverless},
  location  = {Davis, CA, USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/3366623.3368135},
}

@Article{Tran2024,
  author   = {Minh-Ngoc Tran and YoungHan Kim},
  journal  = {Future Generation Computer Systems},
  title    = {Optimized resource usage with hybrid auto-scaling system for knative serverless edge computing},
  year     = {2024},
  issn     = {0167-739X},
  pages    = {304-316},
  volume   = {152},
  abstract = {In the most popular serverless platform - Knative, dynamic resource allocation is implemented using horizontal auto-scaling algorithms to create or delete service instances based on different monitored metrics. However, the assigned resources for each instance are fixed. Vertical scaling up or down assigned resources per instance is required to avoid over-provisioning resources which are limited at the edge. Hybrid (horizontal and vertical) auto-scaling solutions proposed by existing works have several limitations. These solutions are optimized for separated services and get degraded performance when applied in a normal environment with multiple concurrent services. Further, most methods make significant changes to the original Knative platform, and have not been considered to be adopted since then. In this article, instead of Knative modification, we developed separated Kubernetes operators and custom resources (CRs) that can assist the Knative auto-scaler with optimal hybrid auto-scaling configurations based on traffic prediction. First, we characterize each service with a profile of different assigned resource levels pairing with their optimal target Knative’s horizontal scaling request concurrency. Then, based on these profiles, we calculate the best-assigned resources level, target concurrency level, and the number of required instances corresponding to each future time step’s predicted traffic. Finally, these configurations are applied to Knative’s default auto-scaler and services’ CR. Experiments done on our testbed compared our solution with a Knative hybrid auto-scaler solution that does not consider the service’s target request concurrency, and the default Knative horizontal auto-scaler. The results show our solution improvements up to 14% and 20% in terms of resource usage, respectively.},
  doi      = {https://doi.org/10.1016/j.future.2023.11.010},
  keywords = {Serverless computing, Edge computing, Horizontal scaling, Vertical scaling, Quality of service, Resource management},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167739X23004156},
}

@inproceedings {216031,author = {Edward Oakes and Leon Yang and Dennis Zhou and Kevin Houck and Tyler Harter and Andrea Arpaci-Dusseau and Remzi Arpaci-Dusseau},title = {{SOCK}: Rapid Task Provisioning with {Serverless-Optimized} Containers},booktitle = {2018 USENIX Annual Technical Conference (USENIX ATC 18)},year = {2018},isbn = {978-1-931971-44-7},address = {Boston, MA},pages = {57--70},url = {https://www.usenix.org/conference/atc18/presentation/oakes},publisher = {USENIX Association},month = jul}

@article{PEREZ201850,title = {Serverless computing for container-based architectures},journal = {Future Generation Computer Systems},volume = {83},pages = {50-59},year = {2018},issn = {0167-739X},doi = {https://doi.org/10.1016/j.future.2018.01.022},url = {https://www.sciencedirect.com/science/article/pii/S0167739X17316485},author = {Alfonso Pérez and Germán Moltó and Miguel Caballer and Amanda Calatrava},keywords = {Cloud computing, Serverless, Docker, Elasticity, AWS lambda},abstract = {New architectural patterns (e.g. microservices), the massive adoption of Linux containers (e.g. Docker containers), and improvements in key features of Cloud computing such as auto-scaling, have helped developers to decouple complex and monolithic systems into smaller stateless services. In turn, Cloud providers have introduced serverless computing, where applications can be defined as a workflow of event-triggered functions. However, serverless services, such as AWS Lambda, impose serious restrictions for these applications (e.g. using a predefined set of programming languages or difficulting the installation and deployment of external libraries). This paper addresses such issues by introducing a framework and a methodology to create Serverless Container-aware ARchitectures (SCAR). The SCAR framework can be used to create highly-parallel event-driven serverless applications that run on customized runtime environments defined as Docker images on top of AWS Lambda. This paper describes the architecture of SCAR together with the cache-based optimizations applied to minimize cost, exemplified on a massive image processing use case. The results show that, by means of SCAR, AWS Lambda becomes a convenient platform for High Throughput Computing, specially for highly-parallel bursty workloads of short stateless jobs.}}

@Comment{jabref-meta: databaseType:bibtex;}
